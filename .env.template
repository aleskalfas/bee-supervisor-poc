# LLM Provider (watsonx/ollama/openai/groq)
LLM_BACKEND="ibm_vllm"

## WatsonX
# WATSONX_API_KEY=""
# WATSONX_PROJECT_ID=""
# WATSONX_MODEL_OPERATOR="meta-llama/llama-3-1-70b-instruct"
# WATSONX_MODEL_SUPERVISOR="meta-llama/llama-3-1-70b-instruct"
# WATSONX_REGION="us-south"

## Ollama
# OLLAMA_HOST="http://0.0.0.0:11434"
# OLLAMA_MODEL_OPERATOR="llama3.1:8b"
# OLLAMA_MODEL_SUPERVISOR="llama3.1:8b"

## OpenAI
# OPENAI_API_KEY=""
# OPENAI_MODEL_OPERATOR="gpt-4o"
# OPENAI_MODEL_SUPERVISOR="gpt-4o"

## Groq
# GROQ_API_KEY=""
# GROQ_MODEL_OPERATOR="llama-3.1-70b-versatile"
# GROQ_MODEL_SUPERVISOR="llama-3.1-70b-versatile"

## Azure OpenAI
# OPENAI_MODEL_OPERATOR="gpt-4o-mini"
# OPENAI_MODEL_SUPERVISOR="gpt-4o-mini"
# OPENAI_API_VERSION="2024-08-01-preview"
# AZURE_DEPLOYMENT_NAME=""
# AZURE_OPENAI_API_KEY=""
# AZURE_OPENAI_ENDPOINT=""

## VertexAI
# VERTEXAI_MODEL_OPERATOR="gemini-1.5-flash-001"
# VERTEXAI_MODEL_SUPERVISOR="gemini-1.5-flash-001"
# VERTEXAI_LOCATION="us-central1"
# VERTEXAI_PROJECT=""

## IBMvLLM
IBM_VLLM_URL=
IBM_VLLM_MODEL_OPERATOR=meta-llama/llama-3-3-70b-instruct
IBM_VLLM_MODEL_SUPERVISOR=meta-llama/llama-3-3-70b-instruct
IBM_VLLM_ROOT_CERT=
IBM_VLLM_CERT_CHAIN=
IBM_VLLM_PRIVATE_KEY=


# Tools
CODE_INTERPRETER_URL="http://127.0.0.1:50081"

# Framework related
BEE_FRAMEWORK_LOG_PRETTY=true
BEE_FRAMEWORK_LOG_LEVEL="info"
BEE_FRAMEWORK_LOG_SINGLE_LINE="false"

BEE_FRAMEWORK_INSTRUMENTATION_ENABLED="true"
OTEL_EXPORTER_OTLP_ENDPOINT="http://127.0.0.1:4002"
OTEL_EXPORTER_OTLP_HEADERS="x-bee-authorization=testing-api-key"

# Discord